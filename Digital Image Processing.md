## Attention:
- When we talk about image, it means Digital image
- In this course, we only care about the gray level and the amount of pixels. We don't care about the location of the pixels since it's given that [[#^6fd971|the location stay the same]]

### Introduction
- In this day and and age, we usually take information my textual (You are seeing words on the screen)
	- But when we talk about images, we take information by visual
- Our eyes is the best vision model we have! And it's the thing we try to gain knowledge of to process digital image (AKA learn how images form in the eye)
#### Digital Image
- A digital image is a representation of **2D image as a finite set of digital values, called pixels**


### Fundamentals
#### Human eye
- The lens focuses light from objects onto the retina, the retina is covered with light receptors called cones and rods
	- Cones are sensitive to color, which enables our eyes color vision
	- Rods are sensitive to low levels of illumination, and do not detect color

#### Brightness adaptation and discrimination
- Human visual system can perceive approx $10^{10}$ different light intensity level
	- However, at any one time we can only discriminate between a much smaller number - that's brightness adaptation
- Similarly, the perceived intensity of a region is related to the light intensities of the region 
- Example: Mach bands - the pixel value reduces from right to left
![[Chevreul-Mach-bands-illusion-3-wherein-around-the-edges-of-regions-of-uniform-2375383330.png]]
- When we move from one bands to another, there's a distinct different/sudden change in intensity, hence the vertical line
- But as human, we see a gradual change in intensity - we adapt that sudden change
	- Why is that? Because the human visual system's tendency to enhance contrasts at edges - leading to our eye adapt to the difference gradually
		- Thus, we have optical illusion (property of human eyes)

#### Physics phenomenon of the eye: Light and the Electromagnetic Spectrum
- Light is a small part within the electromagnetic spectrum that can be seen by the human eye
	- Thus we only pay attention to digital images that refract light
	- When we change the length of the wave, we also change its property
- The color that we see are determined by the nature of the light reflected from an object

#### Sampling, Quantization & Resolution
#### Image representation
- From the computer POV, the image is a set of pixels within the 2D
	- Each pixels has a coordinates, and a value (AKA grey level)
		- grey level range from 0 (black) - 255 (white)
			- In some case, we can use 0 (black) - 1 (white), with something like .5 (grey) ^296633
			- Why? There are 256 values to choose = $2^{8}$ bits of values = 1 bytes
			- It is to efficiently store image
		- We use the function f(row, col) to represent the pixel
1) Image acquisition
	- Images are generated by illuminating a scene and absorbing the energy reflected by the objects in that scene
2) Image Sampling and Quantization
	- A digital sensor can only measure a limited number of samples at a discrete set of energy level
	- Quantization is the process of **converting a continuous analog signal into a digital representation of this signal**
	![[Pasted image 20250409111123.png]]
	- Sampling: Converting continuous space to discrete samples
		- Intensity values along the line Aâ€“B are **sampled at regular intervals**
		- The curve show the intensity variation from A to B
		- Small square show the intensity values at each sampled point
	- Quantization: Mapping continuous intensity to discrete levels
		- The sampled intensity are now **quantized** into a limited number of level (dashed line on the right - also the grayscale bar)
		- Each sampled intensity is mapped to the nearest discrete level (the square is align with the dashed line)
		- Outcome: The continuous intensity values are now fully converted to discrete digital values
	- A digital image is always just an approx of a real world scene
		- And that approx is close to reality/looks like reality depending on the resolution
3) Spatial Resolution
	- The spatial resolution of an image is determined by how sampling was carried out
	- Spatial resolution simply refers to the **smallest discernable detail** in an image
		- We, as an user, don't care about how an image is sampled or quantized. We care about how the image looks
	- The less amount of pixels we use, the worse the image look
		- Thus, spatial resolution is used to represent the quality of the image
4) Intensity Level Resolution
	- Intensity level resolution refers to the number of intensity levels used to represent the image
		- As we mentioned before on the intensity level, each sampled intensity is mapped to the nearest discrete level
		- And the more intensity levels used, the quantization also gets finer/thinner, the intensity level is also closer together -> The color difference between level is less noticeable -> The finer the level of detail discernable in an image


### Histogram Processing
We will look at image enhancement techniques working in the spatial domain
Image enhancement: The process of making images more useful, this mean:
- Highlight detail in images
- Removing noise from images
- Making image more visually appealing
NOTE: an image maybe visible to the computer "eye", but not for the human eye and vice versa. Thus there are different standard given for image enhancement for different eye

About grey levels
- We known the value of grey level are in the range (0, 255)
	- Though this is only from the display technology POV, as images are typically store in bytes
- For many of the image processing operation, we will use a [[#^296633|different range]]

There are two broad categories of image enhancement techniques
- Spatial domain techniques
	- Direct manipulation of pixels
- Frequency domain techniques
	- Manipulation of Fourier Transform or wavelet transform of an image

**Histogram**: the <label class="ob-comment" title="" style=""> distribution <input type="checkbox"> <span style=""> Related to CDF (Statistic, Probability) </span></label> of gray intensity over all pixels in the images, with:
- r = gray level of a pixel
- n = the no. of pixels with intensity r
- L = the highest gray level


### Point Processing
#### Basic Spatial Domain Image Enhancement
- Most spatial domain enhancement operations can be reduced to the form$$g(x,y)=T[f(x,y)]$$
	- Where $f(x,y)$ is the Input Image
	- $g(x,y)$ is the processed image
	- $T$ is some operation defined over neighborhood $(x,y)$
- This can be understand and dumb down to: The simplest operations occur when the neighborhood is simply the pixel itself$$s=T(r)$$
	- $f(x,y)=r$ | The input image is a single pixel, same goes for output
	- $T$ is known as a **grey level transformation function or a point processing operation**

- The visual content of an image cannot be perceived by seeing each pixels alone:
	- One small reason is that our eyes cannot see each pixels
	- But mostly because an image comes from the natural visual content, we perceive color in a range

- Note: Untouchable location ^6fd971
	- Within image processing, a rule goes by: The pixel location stay the same throughout the process (from input to output)
	- Only the value (grey level) of a pixel can change 

- Negative image
	- Negative images are useful for enhancing white or grey detail embedded in dark regions of an image
	- We essentially take the complement of the image 
![[Pasted image 20250416091735.png]]
$$s = intensity_{max}-r$$

- Thresholding
	- Useful for isolating an object/place of interest from a background
![[Pasted image 20250515123345.png]]


![[Pasted image 20250416092731.png]]
- Left: Logarithmic transformation
- Right: Thresholding function
	- k = the threshold

- $s = c * log(1+r)$
	- c = 1 (smoothing parameter)


### Image Enhancement
#### Spatial Filtering: Neighbors Operation 
- Rather than operating on a single pixels, Neighborhood operations operate on the area (can be a rectangle or triangle) around the center pixels

- Simple neighborhood operations:
	- Min: Set pixels value to min in the neighborhood
	- Max: Set pixels value to max in the neighborhood
	- Median: The midpoint value in a set
		- E.g. We got a set $[1, 7, 15, 18, 24]$ then the median is 15
		- If there are 2 value at the middle, we take the average of them
		- Sometimes the median works better than the average
- The neighborhood spatial filtering goes like this:![[Pasted image 20250423092506.png]]
- The value $e_{processed}$ is the new grey value of the location of $e$ 

- Smoothing spatial filter
	- A filter where we **average** all the pixels in a neighborhood around a central value
	- Useful in removing noise from images and highlighting gross detail
![[Pasted image 20250423092914.png|Simple Averaging Filter]] ^63924c
- Depending on the size of the filter, the detail of an image will become more blurry
![[Pasted image 20250515124901.png]]
- Weighted smoothing filter is a more effective method by llowing different pixels in the neighbourhood different weights in the averaging function
![[Pasted image 20250515130412.png]]




### Image Enhancement #2
#### Spatial Filtering: Sharpening
- Smoothing filter >< Sharpening filter
	- If the smoothing filter impact on the image as a whole (gross detail), the sharpening filter impact on the small detail within the picture
	- Sharpening filter highlight fine detail by removing blurring and highlight edges

#### Spatial Differentiation
- Differentiation measure the rate of change of a function
##### 1st Derivative
$$\frac{\partial f}{\partial x}=f(x+1)-f(x)$$
- Show the difference between subsequent value and measures the ROC of function
- In theory, the value of change (AKA +1) can be any value, so why do we choose +1?
	- E.g. A 3x3 image, with horizontal x axis and vertical y axis
	- When we measure the change, we want to observe a pixel and the change of subsequent pixel
	- y
	- |   |   |   |
	- |   | You are here |  You want to measure this |
	- |   |   |   | x
![[Pasted image 20250425103954.png]]

##### 2nd Derivative
$$\frac{\partial^{2} f}{\partial^{2} x}=[f(x+1)-f(x)]-[f(x)-f(x-1)]$$
$$=f(x+1)+f(x-1)-2f(x)$$
- Instead of measuring from just the subsequent value, 2nd derivative also take the previous value into account
	- y
	- |   |   |   |
	- | You measure this also  | You are here |  You want to measure this |
	- |   |   |   | x
![[Pasted image 20250425104010.png]]

### Using Second Derivatives for Images Enhancement
- More useful than 1st derivative
	- Stronger response to fine detail
	- Simpler implementation
#### Laplacian
- Check slide for more detail
$$\nabla^{2}f=\frac{\partial^2f}{\partial^2x}+\frac{\partial^2f}{\partial^2y}$$
$$=[f(x +1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)]-4f(x,y)$$


### Image Enhancement #3
#### Filtering in Frequency Domain
- The main idea goes as follow:
	- Any **function that periodically repeats itself can be expressed as a sum of sines and cosines of different frequencies** each multiplied by a different coefficient - AKA a Fourier 
	- [ChatGPT](https://chatgpt.com/share/681ef600-62ac-800b-a64e-bc0e26eadd8c)
#### Discrete Fourier Transform (DFT)
- The DFT of $f(x, y)$, for
	- $x=0,1,2,3,4,...,M-1$
	- $y=0,1,2,3,4,...,N-1$
- Denoted bu $F(u,v)$ is given by the equation:
$$F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}f(x,y)e^{-j2\pi(\frac{ux}{M}+\frac{vy}{N})}$$
for $u=0,1,2,...,M-1$ and $v=0,1,2,...,N-1$

Essentially, we are turning image from a 2D signal in Spatial Domain to the sum of sine and cosine function (not wave) in Frequency Domain
![[Pasted image 20250510140502.png]]

#### Inverse DFT
$$f(x,y)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}F(u,v)e^{j2\pi(\frac{ux}{M}+\frac{vy}{N})}$$
for $x=0,1,2,...,M-1$ and $y=0,1,2,...,N-1$

#### The DTF and Image Processing
- To filter an image in the frequency domain:
	- Compute $F(u,v)$ the DFT of the image
	- Multiply $F(u,v)$ by a filter function $F(u,v)$
	- Compute the inverse FPT of the result
![[Pasted image 20250510145114.png]]
- As we can see:
	- The Low Pass Filter works similarly to the [[#^63924c|Smoothing Filter]]
		- Also mean that the filter can be used to blur image
	- The High Pass Filter works similarly to the Sharpening Filter

#### The Low Pass Filter
![[Pasted image 20250510150418.png]]
- When we looks at this image, we don't know whether which part have what frequency. But we can determine which part should be kept or removed
- Remember the filter, we essentially slap 2 image together, and which part is under which threshold will be removed
- The way this filter works is by cut off all high frequency components that are a specified distance $D_{0}$ from the origin og the transform
- In change the size of the filter, we make the circle bigger


White circle = low frequency
Prove by contradiction, if the white circle is high frequency, that means everything within the large outer circle is removed
Thus, if we apply ideal low pass filter with radius of 5, the image cannot be blurry

#### The High Pass Filter
- Opposite to the Low Pass Filter, we remove the region inside the circle and keep the outside


## Morphological Image Processing
#### Foreword
- In the previous lecture, we have studied about Image Segmentation. Now after Segmentation, Morphological Operations can be used to remove imperfections in the segmented image

#### 0 and 1?
- A note before the lecture:
	- In older lecture, we have known 0 as White and 1 as Black
	- But in this lecture, 0 and 1 is quite interchangeable
- Thus, the knowledge below will assume segmentation had already taken place; and images are now made up of 0s for Background Pixels and 1 for Object Pixels
	- AKA we forget 0 and 1 stand for what color

#### Definition
- Morphological Image Processing (AKA Morphology)
	- Describe a **range of image processing techniques that deal with the shape of features in an image**
	- Morphological Operations are typically applied to **remove imperfections introduced during segmentation**
	- Typically operate on bi-level images
		- Bi-level: Background and Objects Pixels
![[Pasted image 20250520133146.png]]

#### Structuring Elements (SE)
- Definition:
	- SE is a small binary matric
	![[Pasted image 20250520133817.png]]E.g. 2x2 matrix
	- It defines a specific **shape and size that is used to probe or interact with the input image**
- SE consist of 2 operations: Hits or Fits
	- Fits: Complete overlap with on pixels
	- Hits: Any overlap with on pixels
- Thus we say that SE is quite similar to Mask/Filter as it moves across the image and interact with (the pixels of) the input image
	- But we do notice a difference
		- In the example: SE have 2x2 matrix, no need for a middle pixels
	- That means: SE can be any size and of any shape

#### Fundamental Operations
- Fundamentally, Morphological IP is similar to Spatial Filtering
- The SE is moved across every pixels in the original image to give a pixels in a new processed image
	- The value of the new pixels depends on the operation performed
- There are 2 basic operation: Erotion and Dilation

##### Erosion
- Erosion of (original) image $f$ by (using) structuring element $s$ is given by$$f \ominus s$$
- The SE $s$ is positioned with its origin at $(x, y)$ and the new pixel value is determined by: $$g(x,y)= \begin{cases} 1 & \text{if } s \text{ fits } f \\ 0 & \text{otherwise} \end{cases}$$
![[Pasted image 20250520140752.png]]
Within the Processed Image, the Blue Pixels are to be kept (= 1) and the Green Pixels are to be removed (= 0)

- So what can we do with Erosion?
![[Pasted image 20250520140912.png]]

##### Dilation
- Dilation of (original) image $f$ by (using) structuring element $s$ is given by$$f \oplus s$$
- The SE $s$ is positioned with its origin at $(x, y)$ and the new pixel value is determined by: $$g(x,y)= \begin{cases} 1 & \text{if } s \text{ hits } f \\ 0 & \text{otherwise} \end{cases}$$
![[Pasted image 20250520142254.png]]
Opposite to Erosion, we make the Processed Image looser than tighter

- So what can we do with Dilation?
![[Pasted image 20250520142503.png]]

#### Compound Operations
- By combining Erosion and Dilation, we can perform a more interesting morphological operation
- The most widely used of these Compound Operations are: Closing and Opening

##### Opening
- The opening of image $f$ by SE $s$ is denoted by:$$f \circ s=(f \ominus s)\oplus s$$Erosion then Dilation
![[Pasted image 20250520143430.png]]

##### Closing
- The closing of image $f$ by SE $s$ is denoted by:$$f \bullet s=(f \oplus s)\ominus s$$Dilation then Erosion
![[Pasted image 20250520143834.png]]

### What do we learned from this lecture
- By utilize every technique of Morphological Processing, more advanced Operations can be implemented
![[Pasted image 20250520144501.png]]